{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_response_by_gpt4(prompt):\n",
    "    headers = {\n",
    "        'Authorization': 'sk-nYN1tlBh0oywCIy6480fDbC8D98d4581Ac6d640bFd8dD96b',\n",
    "        'Content-Type': 'application/json',\n",
    "    }\n",
    "    json_data = {\n",
    "        'model': 'gpt-3.5-turbo',\n",
    "        'messages': [\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': prompt,\n",
    "            },\n",
    "        ],\n",
    "        'stream': False,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post('https://wdapi7.61798.cn/v1/chat/completions',\n",
    "                                headers=headers,\n",
    "                                json=json_data)\n",
    "        result = response.json()\n",
    "        return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except BaseException:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"你是一个专业的中文对话文本句法依存分析语言专家，现在需要你来进行一些对话文本句法依存分析，以下是具体要求：\n",
    "1. 输入的对话包含多轮，你需要对每一轮，每个词进行依存分析（每个词都存在依赖关系，不要遗漏！！！！），这些依赖关系可能跨多轮，输入的格式为：turn_编号:words_0_0,words_0_1,....words_x_x。\n",
    "2. 输出是句法依存信息，格式为：(words_0_0,words_0_2,依赖关系)<>(words_0_1,words_0_3,依赖关系)<>...\n",
    "3. 已知的依赖关系有（\":\"前边是标签名称,\":\"后面是标签的解释）：root:root, sasubj-obj:same subject, object, sasubj:same subject, dfsubj:different subject, subj:subject, subj-in: inner subject, obj:object\n",
    "pred:predicate, att:attribute modifier, adv:adverbial modifier, cmp:complement modifier, coo:coordination, pobj:preposition object, iobj:indirect-object, de:de-construction, adjct:adjunct, app:appellation, exp:explanation, punc:punctuation, frag:fragment, repet:repetition\n",
    "attr:attribution, bckg:background, cause:cause, comp:comparison, cond:condition, cont:contrast, elbr:elaboration, enbm:enablement, eval:evaluation, expl:explanation, joint:joint, manner:manner-means, rstm:restatement, temp:temporal, tp-chg:topic-change, prob-sol:problem-solution, qst-ans:question-answer, stm-rsp:statement-response, req-proc:requirement-process\n",
    "\n",
    "以下是几个示例供你参考：\n",
    "示例一：\n",
    "{{example_01}}\n",
    "\n",
    "示例二：\n",
    "{{example_02}}\n",
    "\n",
    "现在给你一个新的中文对话请你抽取其中的句法依存信息，再次强调输出格式为：(words_0_0,words_0_2,依赖关系)<>(words_0_1,words_0_3,依赖关系)<>...。\n",
    "新对话：\n",
    "{{dialog}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_50.json\", \"r\", encoding=\"utf8\") as fi_train:\n",
    "    train_data = json.load(fi_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入:turn_编号:words_0_0,words_0_1,....words_x_x。\n",
    "# 输出:(words_0_0,words_0_2,依赖关系)<|reserved003|>(words_0_1,words_0_3,依赖关系)<|reserved003|>...\n",
    "examples = []\n",
    "for once_item in train_data:\n",
    "    once_dialog_turns = once_item[\"dialog\"]\n",
    "    output_words_label = once_item[\"relationship\"]\n",
    "    input_text = \"\"\n",
    "    turn_word_idx2words = {}\n",
    "    for turn_idx, once_turn_words in enumerate(once_dialog_turns):\n",
    "        input_text += f\"turn_{turn_idx}:\"\n",
    "        words_text = []\n",
    "        for word_idx, once_word in enumerate(\n",
    "                once_turn_words[\"utterance\"].split()):\n",
    "            words_text.append(f\"{once_word}_{turn_idx}_{word_idx}\")\n",
    "            turn_word_idx2words[f\"{turn_idx}_{word_idx}\"] = once_word\n",
    "        input_text = input_text + \",\".join(words_text) + \"<n>\"\n",
    "    input_text = input_text.strip(\"<n>\")\n",
    "    output_text = []\n",
    "    for once_rel in once_item[\"relationship\"]:\n",
    "        once_rel[0] = once_rel[0].replace(\"-\", \"_\")\n",
    "        once_rel[2] = once_rel[2].replace(\"-\", \"_\")\n",
    "\n",
    "        if once_rel[0] not in turn_word_idx2words or once_rel[\n",
    "                2] not in turn_word_idx2words:\n",
    "            continue\n",
    "        once_word_0 = turn_word_idx2words[once_rel[0]]\n",
    "        once_word_1 = turn_word_idx2words[once_rel[2]]\n",
    "        output_text.append(\n",
    "            f\"({once_word_0}_{once_rel[0]},{once_word_1}_{once_rel[2]},{once_rel[1]})\"\n",
    "        )\n",
    "\n",
    "    output_text = \"<>\".join(output_text)\n",
    "    examples.append(f\"INPUT:{input_text}\\nOUTPUT:{output_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载新的对话数据集\n",
    "with open(\"DuLeMon/both/dev.json\", \"r\",\n",
    "          encoding=\"utf-8\") as fi_new_conversation:\n",
    "    new_conversation_data = []\n",
    "    for once_line in fi_new_conversation:\n",
    "        conversation_ = json.loads(once_line)[\"conversation\"]\n",
    "        conversation_ = [_[4:].strip().split(\"\\t\")[0] for _ in conversation_]\n",
    "        new_conversation_data.append(conversation_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_datas = []\n",
    "idx = 0\n",
    "for once_conversation in tqdm(new_conversation_data[:500]):\n",
    "    print(\"\\n\".join(once_conversation))\n",
    "    dialog = \"\"\n",
    "    for turn_idx, once_turn in enumerate(once_conversation):\n",
    "        words_text = []\n",
    "        for word_idx, once_word in enumerate(once_turn.split(\" \")):\n",
    "            words_text.append(f\"{once_word}_{turn_idx}_{word_idx}\")\n",
    "        dialog += f\"turn_{turn_idx}:\" + \",\".join(words_text) + \"<n>\"\n",
    "    dialog = dialog.strip(\"<n>\")\n",
    "    dialog = f\"INPUT:{dialog}\\nOUTPUT:\"\n",
    "    random_samples = random.choices(examples, k=2)\n",
    "    input_text = PROMPT_TEMPLATE.replace(\"{{example_01}}\",\n",
    "                                         random_samples[0]).replace(\n",
    "                                             \"{{example_02}}\",\n",
    "                                             random_samples[1])\n",
    "    input_text = input_text.replace(\"{{dialog}}\", dialog)\n",
    "\n",
    "    output_text = get_response_by_gpt4(input_text)\n",
    "    print(output_text)\n",
    "    if len(output_text) == 0:\n",
    "        continue\n",
    "    labeled_datas.append([idx, once_conversation, dialog, output_text])\n",
    "    idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(\"dev_labeled_datas.pkl\", \"wb\") as fo_labeled_datas:\n",
    "    pkl.dump(labeled_datas, fo_labeled_datas)\n",
    "len(labeled_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
